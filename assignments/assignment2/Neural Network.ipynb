{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"D:/DS/DL/dlcourse_ai/assignments/data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 3072)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6159940625648515\n",
      "0.0\n",
      "2.7625252407403873\n",
      "0.0\n",
      "-0.506034704450542\n",
      "-0.9430251923348009\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004189480948355722\n",
      "0.0017633776701851676\n",
      "-0.000349979410859888\n",
      "7.82747411592205e-05\n",
      "-0.0006122931434440759\n",
      "-0.001099466753707798\n",
      "Gradient check passed!\n",
      "-1.6743069775162696\n",
      "2.1531721106768025\n",
      "-0.1589188520646767\n",
      "-0.5695812911252207\n",
      "3.348573984491773\n",
      "-4.306389440039866\n",
      "0.31789861908715356\n",
      "1.1391638888210915\n",
      "-0.3625945391739007\n",
      "5.458405109969901\n",
      "-3.3777142853522872\n",
      "-0.8933992309364049\n",
      "Gradient check passed!\n",
      "-0.6328440006346351\n",
      "-0.3900283198139675\n",
      "1.7534099466421682\n",
      "0.5108136597020349\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "# check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "# check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.394657982634321"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_with_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.585669, Train accuracy: 0.231556, val accuracy: 0.256000\n",
      "Loss: 4.548022, Train accuracy: 0.285222, val accuracy: 0.303000\n",
      "Loss: 4.379076, Train accuracy: 0.331667, val accuracy: 0.345000\n",
      "Loss: 4.227915, Train accuracy: 0.377556, val accuracy: 0.384000\n",
      "Loss: 4.185072, Train accuracy: 0.434333, val accuracy: 0.422000\n",
      "Loss: 4.129839, Train accuracy: 0.476111, val accuracy: 0.468000\n",
      "Loss: 3.985788, Train accuracy: 0.500667, val accuracy: 0.487000\n",
      "Loss: 3.838242, Train accuracy: 0.542111, val accuracy: 0.531000\n",
      "Loss: 3.727500, Train accuracy: 0.561000, val accuracy: 0.544000\n",
      "Loss: 3.668556, Train accuracy: 0.591889, val accuracy: 0.576000\n",
      "Loss: 3.640082, Train accuracy: 0.614778, val accuracy: 0.597000\n",
      "Loss: 3.624651, Train accuracy: 0.623000, val accuracy: 0.601000\n",
      "Loss: 3.243839, Train accuracy: 0.635667, val accuracy: 0.622000\n",
      "Loss: 3.263682, Train accuracy: 0.647778, val accuracy: 0.624000\n",
      "Loss: 3.314525, Train accuracy: 0.663000, val accuracy: 0.642000\n",
      "Loss: 3.339706, Train accuracy: 0.668444, val accuracy: 0.652000\n",
      "Loss: 3.219863, Train accuracy: 0.674000, val accuracy: 0.635000\n",
      "Loss: 3.169978, Train accuracy: 0.682556, val accuracy: 0.650000\n",
      "Loss: 3.191770, Train accuracy: 0.688111, val accuracy: 0.660000\n",
      "Loss: 3.123801, Train accuracy: 0.694556, val accuracy: 0.664000\n",
      "Loss: 3.072015, Train accuracy: 0.704333, val accuracy: 0.687000\n",
      "Loss: 2.941665, Train accuracy: 0.707778, val accuracy: 0.680000\n",
      "Loss: 2.986059, Train accuracy: 0.716222, val accuracy: 0.690000\n",
      "Loss: 3.048103, Train accuracy: 0.718333, val accuracy: 0.688000\n",
      "Loss: 2.800645, Train accuracy: 0.724444, val accuracy: 0.703000\n",
      "Loss: 2.878182, Train accuracy: 0.728778, val accuracy: 0.696000\n",
      "Loss: 2.966142, Train accuracy: 0.733889, val accuracy: 0.706000\n",
      "Loss: 2.720146, Train accuracy: 0.738333, val accuracy: 0.703000\n",
      "Loss: 2.756484, Train accuracy: 0.739667, val accuracy: 0.705000\n",
      "Loss: 2.693291, Train accuracy: 0.744000, val accuracy: 0.707000\n",
      "Loss: 2.643541, Train accuracy: 0.749333, val accuracy: 0.708000\n",
      "Loss: 2.482045, Train accuracy: 0.749667, val accuracy: 0.707000\n",
      "Loss: 2.596677, Train accuracy: 0.755444, val accuracy: 0.709000\n",
      "Loss: 2.599107, Train accuracy: 0.762222, val accuracy: 0.716000\n",
      "Loss: 2.449573, Train accuracy: 0.759667, val accuracy: 0.713000\n",
      "Loss: 2.502012, Train accuracy: 0.762778, val accuracy: 0.715000\n",
      "Loss: 2.490025, Train accuracy: 0.769222, val accuracy: 0.723000\n",
      "Loss: 2.384650, Train accuracy: 0.772889, val accuracy: 0.721000\n",
      "Loss: 2.580332, Train accuracy: 0.771556, val accuracy: 0.723000\n",
      "Loss: 2.524264, Train accuracy: 0.774222, val accuracy: 0.720000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 0.000807)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 0.042)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x33573790>]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.256472, Train accuracy: 0.517667, val accuracy: 0.487000\n",
      "Loss: 3.615439, Train accuracy: 0.656667, val accuracy: 0.637000\n",
      "Loss: 3.678177, Train accuracy: 0.671000, val accuracy: 0.640000\n",
      "Loss: 2.423599, Train accuracy: 0.689333, val accuracy: 0.671000\n",
      "Loss: 2.417370, Train accuracy: 0.728889, val accuracy: 0.685000\n",
      "Loss: 2.056477, Train accuracy: 0.684556, val accuracy: 0.637000\n",
      "Loss: 2.256100, Train accuracy: 0.740111, val accuracy: 0.696000\n",
      "Loss: 1.456663, Train accuracy: 0.775333, val accuracy: 0.721000\n",
      "Loss: 1.546658, Train accuracy: 0.761000, val accuracy: 0.709000\n",
      "Loss: 1.439180, Train accuracy: 0.776333, val accuracy: 0.713000\n",
      "Loss: 1.181202, Train accuracy: 0.796556, val accuracy: 0.722000\n",
      "Loss: 1.775260, Train accuracy: 0.815111, val accuracy: 0.735000\n",
      "Loss: 1.274355, Train accuracy: 0.813556, val accuracy: 0.743000\n",
      "Loss: 1.309635, Train accuracy: 0.804556, val accuracy: 0.724000\n",
      "Loss: 1.218307, Train accuracy: 0.806889, val accuracy: 0.714000\n",
      "Loss: 1.347026, Train accuracy: 0.826222, val accuracy: 0.746000\n",
      "Loss: 1.209142, Train accuracy: 0.820111, val accuracy: 0.743000\n",
      "Loss: 1.008379, Train accuracy: 0.818778, val accuracy: 0.721000\n",
      "Loss: 1.304154, Train accuracy: 0.786556, val accuracy: 0.718000\n",
      "Loss: 1.286956, Train accuracy: 0.828000, val accuracy: 0.729000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 0.001)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.354969, Train accuracy: 0.508778, val accuracy: 0.520000\n",
      "Loss: 2.875932, Train accuracy: 0.616111, val accuracy: 0.595000\n",
      "Loss: 2.303637, Train accuracy: 0.666333, val accuracy: 0.626000\n",
      "Loss: 1.933942, Train accuracy: 0.687667, val accuracy: 0.639000\n",
      "Loss: 1.643221, Train accuracy: 0.719556, val accuracy: 0.671000\n",
      "Loss: 1.557628, Train accuracy: 0.718333, val accuracy: 0.677000\n",
      "Loss: 1.496490, Train accuracy: 0.655333, val accuracy: 0.593000\n",
      "Loss: 1.278581, Train accuracy: 0.746111, val accuracy: 0.705000\n",
      "Loss: 1.198933, Train accuracy: 0.740444, val accuracy: 0.691000\n",
      "Loss: 1.440872, Train accuracy: 0.734667, val accuracy: 0.674000\n",
      "Loss: 1.250073, Train accuracy: 0.776889, val accuracy: 0.694000\n",
      "Loss: 1.349309, Train accuracy: 0.774000, val accuracy: 0.715000\n",
      "Loss: 1.331557, Train accuracy: 0.747444, val accuracy: 0.681000\n",
      "Loss: 1.201776, Train accuracy: 0.740222, val accuracy: 0.693000\n",
      "Loss: 1.454083, Train accuracy: 0.726333, val accuracy: 0.659000\n",
      "Loss: 1.254404, Train accuracy: 0.738222, val accuracy: 0.669000\n",
      "Loss: 1.038189, Train accuracy: 0.768667, val accuracy: 0.693000\n",
      "Loss: 1.178717, Train accuracy: 0.765667, val accuracy: 0.698000\n",
      "Loss: 1.309111, Train accuracy: 0.793000, val accuracy: 0.707000\n",
      "Loss: 1.258522, Train accuracy: 0.752000, val accuracy: 0.698000\n",
      "Loss: 1.014574, Train accuracy: 0.791222, val accuracy: 0.718000\n",
      "Loss: 1.235831, Train accuracy: 0.793778, val accuracy: 0.735000\n",
      "Loss: 1.091461, Train accuracy: 0.797444, val accuracy: 0.710000\n",
      "Loss: 1.285664, Train accuracy: 0.789111, val accuracy: 0.713000\n",
      "Loss: 1.034321, Train accuracy: 0.768889, val accuracy: 0.688000\n",
      "Loss: 1.116986, Train accuracy: 0.787222, val accuracy: 0.705000\n",
      "Loss: 1.075718, Train accuracy: 0.797778, val accuracy: 0.716000\n",
      "Loss: 1.195921, Train accuracy: 0.790667, val accuracy: 0.714000\n",
      "Loss: 1.027746, Train accuracy: 0.751667, val accuracy: 0.687000\n",
      "Loss: 1.169627, Train accuracy: 0.819111, val accuracy: 0.728000\n",
      "Loss: 1.201453, Train accuracy: 0.771222, val accuracy: 0.665000\n",
      "Loss: 1.200491, Train accuracy: 0.759222, val accuracy: 0.659000\n",
      "Loss: 1.010734, Train accuracy: 0.798000, val accuracy: 0.724000\n",
      "Loss: 0.951729, Train accuracy: 0.800778, val accuracy: 0.697000\n",
      "Loss: 1.132547, Train accuracy: 0.822333, val accuracy: 0.734000\n",
      "Loss: 1.018525, Train accuracy: 0.787667, val accuracy: 0.700000\n",
      "Loss: 1.058744, Train accuracy: 0.809889, val accuracy: 0.728000\n",
      "Loss: 1.088701, Train accuracy: 0.826667, val accuracy: 0.722000\n",
      "Loss: 0.945499, Train accuracy: 0.837222, val accuracy: 0.721000\n",
      "Loss: 0.855351, Train accuracy: 0.788778, val accuracy: 0.697000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 0.000807)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=0.1, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 286.753420, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 253.678574, Train accuracy: 0.533333, val accuracy: 0.133333\n",
      "Loss: 224.642433, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 198.906486, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 176.514140, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 156.348954, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 138.405786, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 122.970461, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 109.167632, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 96.866246, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 85.808988, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 76.023805, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 67.781010, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 60.040251, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 53.234036, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 47.337403, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 42.242180, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 37.430665, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 33.436218, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 29.710341, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 26.379785, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 23.552381, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 20.947379, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 18.792390, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 16.811274, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 15.042458, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 13.439468, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 12.132912, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 10.880571, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 9.578373, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 8.596949, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 8.048852, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 7.162136, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 6.443094, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 5.901765, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 5.211349, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 4.921811, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 4.647733, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 4.033699, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.793509, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.488642, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.255698, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 3.030207, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.865718, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.664526, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.423016, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.400092, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.119179, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 2.067068, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.966460, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.804051, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.915339, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.673367, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.733825, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.805532, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.728111, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.564382, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.618780, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.683359, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.441073, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.237512, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.345613, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.417158, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.333635, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.446430, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.262644, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.255579, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.353915, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.164117, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.346867, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.413148, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.263705, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.288262, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.261160, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.267500, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.265907, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.119036, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.243991, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.322396, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.111393, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.169296, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.257104, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.196990, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.077679, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.180040, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.154765, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.097016, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.244765, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.163196, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.124886, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.180733, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.095048, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.184861, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.207999, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.175646, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.202143, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.136720, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.312562, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.208544, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.281740, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.242055, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.160323, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.176998, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.176973, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.292365, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.229457, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.095339, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.170786, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.211301, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.095100, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.208422, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.209053, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.209143, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.240650, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.243093, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.022168, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.178834, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.231138, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.327394, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.997702, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.129662, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.209624, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.360720, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.243140, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.131861, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.306325, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.195834, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.218365, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.126843, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.045726, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.168703, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.261051, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.135959, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.225228, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.242946, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.321187, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.164017, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.136495, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.278163, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.052972, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.206616, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.270030, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.161550, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.037839, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.335856, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.221783, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.163109, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.991520, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.183782, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.031154, Train accuracy: 1.000000, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.497840, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 5.245360, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 3.969691, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 3.976704, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.697392, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.498816, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.236056, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.323540, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.251132, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.294492, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.159138, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.155897, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.167544, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.130142, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.124365, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.104123, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.111498, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.085727, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.082114, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 3.063576, Train accuracy: 1.000000, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count #: 1.000000\n",
      "Loss: 9.387828, Train accuracy: 0.317222, val accuracy: 0.319000\n",
      "Loss: 7.771269, Train accuracy: 0.488111, val accuracy: 0.484000\n",
      "Loss: 6.620972, Train accuracy: 0.579111, val accuracy: 0.563000\n",
      "Loss: 5.485840, Train accuracy: 0.634667, val accuracy: 0.605000\n",
      "Loss: 4.869850, Train accuracy: 0.660111, val accuracy: 0.650000\n",
      "Count #: 2.000000\n",
      "Loss: 18.000279, Train accuracy: 0.199889, val accuracy: 0.205000\n",
      "Loss: 16.698258, Train accuracy: 0.235556, val accuracy: 0.242000\n",
      "Loss: 15.559687, Train accuracy: 0.288667, val accuracy: 0.282000\n",
      "Loss: 14.372253, Train accuracy: 0.327778, val accuracy: 0.331000\n",
      "Loss: 13.521526, Train accuracy: 0.375778, val accuracy: 0.372000\n",
      "Count #: 3.000000\n",
      "Loss: 3.403294, Train accuracy: 0.414889, val accuracy: 0.416000\n",
      "Loss: 3.016582, Train accuracy: 0.570444, val accuracy: 0.540000\n",
      "Loss: 2.576773, Train accuracy: 0.648333, val accuracy: 0.637000\n",
      "Loss: 2.448297, Train accuracy: 0.685111, val accuracy: 0.661000\n",
      "Loss: 2.540553, Train accuracy: 0.717333, val accuracy: 0.686000\n",
      "Count #: 4.000000\n",
      "Loss: 4.049070, Train accuracy: 0.257556, val accuracy: 0.246000\n",
      "Loss: 3.633260, Train accuracy: 0.392444, val accuracy: 0.356000\n",
      "Loss: 3.546956, Train accuracy: 0.471111, val accuracy: 0.453000\n",
      "Loss: 3.212369, Train accuracy: 0.555444, val accuracy: 0.531000\n",
      "Loss: 3.042123, Train accuracy: 0.606778, val accuracy: 0.578000\n",
      "Count #: 5.000000\n",
      "Loss: 3.223048, Train accuracy: 0.461000, val accuracy: 0.486000\n",
      "Loss: 2.014266, Train accuracy: 0.494111, val accuracy: 0.473000\n",
      "Loss: 1.867316, Train accuracy: 0.549222, val accuracy: 0.548000\n",
      "Loss: 1.885836, Train accuracy: 0.520889, val accuracy: 0.518000\n",
      "Loss: 1.939523, Train accuracy: 0.563111, val accuracy: 0.563000\n",
      "Count #: 6.000000\n",
      "Loss: 2.622972, Train accuracy: 0.186556, val accuracy: 0.188000\n",
      "Loss: 2.588854, Train accuracy: 0.210111, val accuracy: 0.219000\n",
      "Loss: 2.602557, Train accuracy: 0.221889, val accuracy: 0.232000\n",
      "Loss: 2.563033, Train accuracy: 0.235667, val accuracy: 0.254000\n",
      "Loss: 2.480290, Train accuracy: 0.260667, val accuracy: 0.265000\n",
      "Count #: 7.000000\n",
      "Loss: 5.086834, Train accuracy: 0.190333, val accuracy: 0.192000\n",
      "Loss: 4.978240, Train accuracy: 0.213222, val accuracy: 0.215000\n",
      "Loss: 4.974856, Train accuracy: 0.232667, val accuracy: 0.226000\n",
      "Loss: 4.884209, Train accuracy: 0.256444, val accuracy: 0.248000\n",
      "Loss: 4.861053, Train accuracy: 0.271889, val accuracy: 0.274000\n",
      "Count #: 8.000000\n",
      "Loss: 24.239538, Train accuracy: 0.224444, val accuracy: 0.222000\n",
      "Loss: 20.709214, Train accuracy: 0.256444, val accuracy: 0.259000\n",
      "Loss: 17.664077, Train accuracy: 0.318444, val accuracy: 0.330000\n",
      "Loss: 15.183900, Train accuracy: 0.370111, val accuracy: 0.375000\n",
      "Loss: 13.026308, Train accuracy: 0.413000, val accuracy: 0.411000\n",
      "Count #: 9.000000\n",
      "Loss: 3.789067, Train accuracy: 0.366778, val accuracy: 0.350000\n",
      "Loss: 3.444148, Train accuracy: 0.579000, val accuracy: 0.553000\n",
      "Loss: 2.869464, Train accuracy: 0.654778, val accuracy: 0.625000\n",
      "Loss: 2.654568, Train accuracy: 0.689000, val accuracy: 0.659000\n",
      "Loss: 2.506667, Train accuracy: 0.716667, val accuracy: 0.674000\n",
      "Count #: 10.000000\n",
      "Loss: 5.182911, Train accuracy: 0.214556, val accuracy: 0.225000\n",
      "Loss: 5.033256, Train accuracy: 0.271000, val accuracy: 0.276000\n",
      "Loss: 4.906865, Train accuracy: 0.319222, val accuracy: 0.331000\n",
      "Loss: 4.756805, Train accuracy: 0.393889, val accuracy: 0.375000\n",
      "Loss: 4.634443, Train accuracy: 0.438667, val accuracy: 0.408000\n",
      "best validation accuracy achieved: 0.686000 at reg_strength 0.000538 and learning_rate  0.029696\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "reg_strength_history = []\n",
    "learning_rate_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "maxim = 10\n",
    "for count in range(maxim):\n",
    "    print('Count #: %f'% (count+1))\n",
    "    reg_strength = 10**np.random.uniform(-4,-2)\n",
    "    learning_rate = 10**np.random.uniform(-3,-1)\n",
    "    model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = reg_strength)\n",
    "    dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "    trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=learning_rate, learning_rate_decay=0.99, num_epochs = 5, batch_size = 100)\n",
    "    \n",
    "    loss, train, val = trainer.fit()\n",
    "    \n",
    "    loss_history.append(loss[-1])\n",
    "    train_history.append(train[-1])\n",
    "    val_history.append(val[-1])\n",
    "    reg_strength_history.append(reg_strength)\n",
    "    learning_rate_history.append(learning_rate)\n",
    "\n",
    "print('best validation accuracy achieved: %f at reg_strength %f and learning_rate  %f' % (np.max(val_history), reg_strength_history[np.argmax(val_history)], learning_rate_history[np.argmax(val_history)])   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.subplot(211)\n",
    "# plt.title(\"Loss\")\n",
    "# plt.plot(loss_history)\n",
    "# plt.subplot(212)\n",
    "# plt.title(\"Train/validation accuracy\")\n",
    "# plt.plot(train_history)\n",
    "# plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.682000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
