{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Word2VecCuda.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UcwOTpk5-fHJ"},"source":["# Задание 5.1 - Word2Vec\n","\n","В этом задании мы натренируем свои word vectors на очень небольшом датасете.\n","Мы будем использовать самую простую версию word2vec, без negative sampling и других оптимизаций.\n","\n","Перед запуском нужно запустить скрипт `download_data.sh` чтобы скачать данные.\n","\n","Датасет и модель очень небольшие, поэтому это задание можно выполнить и без GPU."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oz33ZeXe_8Ah","executionInfo":{"status":"ok","timestamp":1637340081309,"user_tz":-120,"elapsed":25988,"user":{"displayName":"Олександр Анатоліїович Савченко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFxEhCEDXBdXMW8HVBZvI3bm-ax-lV7gibeoHr=s64","userId":"06649945919620007057"}},"outputId":"5ca2a794-922d-4efc-ab37-6718b967dbef"},"source":["#@title\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"id":"kpuNQWq5-fHO","executionInfo":{"status":"ok","timestamp":1637340111807,"user_tz":-120,"elapsed":26803,"user":{"displayName":"Олександр Анатоліїович Савченко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFxEhCEDXBdXMW8HVBZvI3bm-ax-lV7gibeoHr=s64","userId":"06649945919620007057"}}},"source":["#@title\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","\n","from torchvision import transforms\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# We'll use Principal Component Analysis (PCA) to visualize word vectors,\n","# so make sure you install dependencies from requirements.txt!\n","from sklearn.decomposition import PCA \n","\n","%matplotlib inline\n","device = torch.device(\"cuda:0\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"s-ONrgDl-fHQ","executionInfo":{"status":"ok","timestamp":1637340115006,"user_tz":-120,"elapsed":1044,"user":{"displayName":"Олександр Анатоліїович Савченко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFxEhCEDXBdXMW8HVBZvI3bm-ax-lV7gibeoHr=s64","userId":"06649945919620007057"}},"outputId":"f160fad6-4da2-414e-9030-a95b2d611057"},"source":["#@title\n","import os\n","import numpy as np\n","\n","class StanfordTreeBank:\n","    '''\n","    Wrapper for accessing Stanford Tree Bank Dataset\n","    https://nlp.stanford.edu/sentiment/treebank.html\n","    \n","    Parses dataset, gives each token and index and provides lookups\n","    from string token to index and back\n","    \n","    Allows to generate random context with sampling strategy described in\n","    word2vec paper:\n","    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n","    '''\n","    def __init__(self):\n","        self.index_by_token = {}\n","        self.token_by_index = []\n","\n","        self.sentences = []\n","\n","        self.token_freq = {}\n","        \n","        self.token_reject_by_index = None\n","\n","    def load_dataset(self, folder):\n","        filename = os.path.join(folder, \"datasetSentences.txt\")\n","\n","        with open(filename, \"r\", encoding=\"latin1\") as f:\n","            l = f.readline() # skip the first line\n","            \n","            for l in f:\n","                splitted_line = l.strip().split()\n","                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n","                    \n","                self.sentences.append(words)\n","                for word in words:\n","                    if word in self.token_freq:\n","                        self.token_freq[word] +=1 \n","                    else:\n","                        index = len(self.token_by_index)\n","                        self.token_freq[word] = 1\n","                        self.index_by_token[word] = index\n","                        self.token_by_index.append(word)\n","        self.compute_token_prob()\n","                        \n","    def compute_token_prob(self):\n","        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n","        words_freq = words_count / np.sum(words_count)\n","        \n","        # Following sampling strategy from word2vec paper:\n","        # https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n","        self.token_reject_by_index = 1- np.sqrt(1e-5/words_freq)\n","    \n","    def check_reject(self, word):\n","        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n","        \n","    def get_random_context(self, context_length=5):\n","        \"\"\"\n","        Returns tuple of center word and list of context words\n","        \"\"\"\n","        sentence_sampled = []\n","        while len(sentence_sampled) <= 2:\n","            sentence_index = np.random.randint(len(self.sentences)) \n","            sentence = self.sentences[sentence_index]\n","            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n","    \n","        center_word_index = np.random.randint(len(sentence_sampled))\n","        \n","        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n","        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n","        \n","        return sentence_sampled[center_word_index], words_before+words_after\n","    \n","    def num_tokens(self):\n","        return len(self.token_by_index)\n","        \n","data = StanfordTreeBank()\n","data.load_dataset(\"/content/drive/My Drive/Colab Notebooks/stanfordSentimentTreebank/stanfordSentimentTreebank/\")\n","\n","print(\"Num tokens:\", data.num_tokens())\n","for i in range(5):\n","    center_word, other_words = data.get_random_context(5)\n","    print(center_word, other_words)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Num tokens: 19538\n","ironies ['schrader', 'relies']\n","quentin ['spawn', 'fools', 'handful', 'branched']\n","counter-cultural [\"n't\", 'definitive', '--', 'makers']\n","swim ['killer', 'whale', 'through']\n","diaz ['kept', 'cameron', 'prisoner', 'cage']\n"]}]},{"cell_type":"markdown","metadata":{"id":"l2Pb8PAW-fHS"},"source":["# Имплеменируем PyTorch-style Dataset для Word2Vec\n","\n","Этот Dataset должен сгенерировать много случайных контекстов и превратить их в сэмплы для тренировки.\n","\n","Напоминаем, что word2vec модель получает на вход One-hot вектор слова и тренирует простую сеть для предсказания на его основе соседних слов.\n","Из набора слово-контекст создается N сэмплов (где N - количество слов в контексте):\n","\n","Например:\n","\n","Слово: `orders` и контекст: `['love', 'nicest', 'to', '50-year']` создадут 4 сэмпла:\n","- input: `orders`, target: `love`\n","- input: `orders`, target: `nicest`\n","- input: `orders`, target: `to`\n","- input: `orders`, target: `50-year`\n","\n","Все слова на входе и на выходе закодированы через one-hot encoding, с размером вектора равным количеству токенов."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TzT3HD2K-fHT","executionInfo":{"status":"ok","timestamp":1637340188348,"user_tz":-120,"elapsed":370,"user":{"displayName":"Олександр Анатоліїович Савченко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFxEhCEDXBdXMW8HVBZvI3bm-ax-lV7gibeoHr=s64","userId":"06649945919620007057"}},"outputId":"c4989073-b2ee-4e25-dd1f-88ae01e74c2e"},"source":["#@title\n","class Word2VecPlain(Dataset):\n","    '''\n","    PyTorch Dataset for plain Word2Vec.\n","    Accepts StanfordTreebank as data and is able to generate dataset based on\n","    a number of random contexts\n","    '''\n","    def __init__(self, data, num_contexts=100):\n","        '''\n","        Initializes Word2VecPlain, but doesn't generate the samples yet\n","        (for that, use generate_dataset)\n","        Arguments:\n","        data - StanfordTreebank instace\n","        num_contexts - number of random contexts to use when generating a dataset\n","        '''\n","        # TODO: Implement what you need for other methods!\n","        self.data = data\n","        self.num_contexts = num_contexts\n","        self.input = []\n","        self.output = []\n","        self.num_tokens = self.data.num_tokens()\n","        \n","    \n","    def generate_dataset(self):\n","        '''\n","        Generates dataset samples from random contexts\n","        Note: there will be more samples than contexts because every context\n","        can generate more than one sample\n","        '''\n","        # TODO: Implement generating the dataset\n","        # You should sample num_contexts contexts from the data and turn them into samples\n","        # Note you will have several samples from one context\n","        for i in range(self.num_contexts):\n","            center_word, other_words = self.data.get_random_context()\n","            for word in other_words:\n","                self.input.append(center_word)\n","                self.output.append(word)\n","        \n","    def __len__(self):\n","        '''\n","        Returns total number of samples\n","        '''\n","        return len(self.input)\n","    \n","    def __getitem__(self, index):\n","        '''\n","        Returns i-th sample\n","        \n","        Return values:\n","        input_vector - torch.Tensor with one-hot representation of the input vector\n","        output_index - index of the target word (not torch.Tensor!)\n","        '''\n","        # TODO: Generate tuple of 2 return arguments for i-th sample \n","        input_vector = torch.zeros([self.data.num_tokens()]).float()\n","        input_word = self.input[index]\n","        word_index_in_data = self.data.index_by_token[input_word]\n","        input_vector[word_index_in_data] = 1\n","        \n","        output_word = self.output[index]\n","        output_index = self.data.index_by_token[output_word]\n","        \n","        return input_vector, output_index\n","        \n","\n","dataset = Word2VecPlain(data, 10)\n","dataset.generate_dataset()\n","input_vector, target = dataset[3]\n","print(\"Sample - input: %s, target: %s\" % (input_vector, int(target))) # target should be able to convert to int\n","assert isinstance(input_vector, torch.Tensor)\n","assert torch.sum(input_vector) == 1.0\n","assert input_vector.shape[0] == data.num_tokens()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample - input: tensor([0., 0., 0.,  ..., 0., 0., 0.]), target: 10347\n"]}]},{"cell_type":"markdown","metadata":{"id":"jPipG29k-fHU"},"source":["# Создаем модель и тренируем ее"]},{"cell_type":"code","metadata":{"id":"F6h0czHo-fHU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637340307271,"user_tz":-120,"elapsed":1697,"user":{"displayName":"Олександр Анатоліїович Савченко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFxEhCEDXBdXMW8HVBZvI3bm-ax-lV7gibeoHr=s64","userId":"06649945919620007057"}},"outputId":"57348be0-44da-4dac-b49e-9c25a6522059"},"source":["#@title\n","# Create the usual PyTorch structures\n","dataset = Word2VecPlain(data, 30000)\n","dataset.generate_dataset()\n","\n","# # We'll be training very small word vectors!\n","wordvec_dim = 10\n","\n","# # We can use a standard sequential model for this\n","nn_model = nn.Sequential(\n","            nn.Linear(dataset.num_tokens, wordvec_dim, bias=False),\n","            nn.Linear(wordvec_dim, dataset.num_tokens, bias=False), \n","         )\n","nn_model.type(torch.FloatTensor)\n","\n","nn_model.to(device)\n"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=19538, out_features=10, bias=False)\n","  (1): Linear(in_features=10, out_features=19538, bias=False)\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"amEb7YDc-fHV","executionInfo":{"status":"ok","timestamp":1637340312853,"user_tz":-120,"elapsed":241,"user":{"displayName":"Олександр Анатоліїович Савченко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFxEhCEDXBdXMW8HVBZvI3bm-ax-lV7gibeoHr=s64","userId":"06649945919620007057"}}},"source":["#@title\n","def extract_word_vectors(nn_model):\n","    '''\n","    Extracts word vectors from the model\n","    \n","    Returns:\n","    input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n","    output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n","    '''\n","    # TODO: Implement extracting word vectors from param weights\n","    # return tuple of input vectors and output vectos \n","    # Hint: you can access weights as Tensors through nn.Linear class attributes\n","    input_vectors = torch.t([param.data for param in nn_model.parameters()][0].clone().detach())\n","    output_vectors = [param.data for param in nn_model.parameters()][1].clone().detach()\n","    return input_vectors, output_vectors\n"," \n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUZCET0e-fHV","executionInfo":{"status":"ok","timestamp":1637340317137,"user_tz":-120,"elapsed":220,"user":{"displayName":"Олександр Анатоліїович Савченко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFxEhCEDXBdXMW8HVBZvI3bm-ax-lV7gibeoHr=s64","userId":"06649945919620007057"}}},"source":["#@title\n","untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n","assert untrained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n","assert untrained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQYg67Fw-fHW","executionInfo":{"status":"ok","timestamp":1637340400303,"user_tz":-120,"elapsed":231,"user":{"displayName":"Олександр Анатоліїович Савченко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFxEhCEDXBdXMW8HVBZvI3bm-ax-lV7gibeoHr=s64","userId":"06649945919620007057"}}},"source":["#@title\n","def train_model(model, dataset, train_loader, optimizer, scheduler, num_epochs):\n","    '''\n","    Trains plain word2vec using cross-entropy loss and regenerating dataset every epoch\n","    \n","    Returns:\n","    loss_history, train_history\n","    '''\n","    \n","    loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n","    \n","    loss_history = []\n","    train_history = []\n","    for epoch in range(num_epochs):\n","        model.train() # Enter train mode\n","        dataset.generate_dataset() # Regenerate dataset every epoch\n","         # TODO Implement training for this model\n","        # Note we don't have any validation set here because our purpose is the word vectors,\n","        # not the predictive performance of the model\n","        # And don't forget to step the learing rate scheduler!\n","        loss_accum = 0\n","        correct_samples = 0\n","        total_samples = 0\n","        for i_step, (x, y) in enumerate(train_loader):\n","            x_gpu = x.to(device)\n","            y_gpu = y.to(device)\n","            prediction = model(x_gpu)    \n","            loss_value = loss(prediction, y_gpu)\n","            optimizer.zero_grad()\n","            loss_value.backward()\n","            optimizer.step()\n","            _, indices = torch.max(prediction, 1)\n","            correct_samples += torch.sum(indices == y_gpu)\n","            total_samples += y_gpu.shape[0]\n","            loss_accum += loss_value\n","\n","        ave_loss = loss_accum / i_step\n","        train_accuracy = float(correct_samples) / total_samples\n","        loss_history.append(float(ave_loss))\n","        train_history.append(train_accuracy)\n","        scheduler.step()      \n","        print(\"Epoch %i, Average loss: %f, Train accuracy: %f\" % (epoch, ave_loss, train_accuracy))\n","        \n","    return loss_history, train_history"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3JuXuZQM-fHX"},"source":["# Ну и наконец тренировка!\n","\n","Добейтесь значения ошибки меньше **8.0**."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"HF9pD_aO-fHY","colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"status":"error","timestamp":1637340819251,"user_tz":-120,"elapsed":377350,"user":{"displayName":"Олександр Анатоліїович Савченко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFxEhCEDXBdXMW8HVBZvI3bm-ax-lV7gibeoHr=s64","userId":"06649945919620007057"}},"outputId":"de0d6c45-e947-450d-ec27-cbc953674572"},"source":["#@title\n","# Finally, let's train the model!\n","\n","# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n","optimizer = optim.SGD(nn_model.parameters(), lr=1.35, weight_decay=0)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n","\n","loss_history, train_history = train_model(nn_model, dataset, train_loader, optimizer, scheduler, 15)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Average loss: 9.876924, Train accuracy: 0.000906\n","Epoch 1, Average loss: 9.834677, Train accuracy: 0.004519\n","Epoch 2, Average loss: 9.579104, Train accuracy: 0.006007\n","Epoch 3, Average loss: 9.180169, Train accuracy: 0.007656\n","Epoch 4, Average loss: 8.822652, Train accuracy: 0.013554\n","Epoch 5, Average loss: 8.514903, Train accuracy: 0.020760\n","Epoch 6, Average loss: 8.285566, Train accuracy: 0.026582\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-1fd3f4d5c0d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-697de1cd5344>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, train_loader, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"JesF2-ho-fHY"},"source":["#@title\n","dataset = Word2VecPlain(data, 30000)\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"od9rveFG-fHZ"},"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","def approx_loss_reg_strength_learning_rate(trainer_func, train_loader, \\\n","                                            loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor), num_epochs = 5, \\\n","                                            num_experiments = 50, reg_strength_10_exp = (-6, -1), learning_rate_10_exp = (-1, 0.5)): \n","    \"\"\"\n","    ONLY FOR: Pytorch TENSORS, CUDA platform, optim.SGD  \n","    \n","    trainer_func - should return loss_history, train_accur_history, val_accur_history BY EPOCHS\n","    loss - for example nn.CrossEntropyLoss().type(torch.FloatTensor)\n","    nmb_experiments, int >= 100\n","    reg_strength_10_exp, tulip (-6, 0)   \n","    learning_rate_10_exp, tulip (-6, 0) \n","    \n","    \n","    \"\"\"\n","    loss_history = [] # history by hyperparams !!! \n","    train_history = [] # history by hyperparams !!!\n","     \n","    reg_strength_history = []\n","    learning_rate_history = []\n","\n","    for count in range(num_experiments):\n","        print('Count #: %d'% (count+1))\n","        reg_strength = 10**np.random.uniform(reg_strength_10_exp[0],reg_strength_10_exp[1]) \n","        learning_rate = 10**np.random.uniform(learning_rate_10_exp[0],learning_rate_10_exp[1])\n","        print('reg_strength: %f'% (reg_strength))\n","        print('learning_rate: %f'% (learning_rate))\n","        wordvec_dim = 10\n","\n","        nn_model = nn.Sequential(\n","            nn.Linear(dataset.num_tokens, wordvec_dim, bias=False),\n","            nn.Linear(wordvec_dim, dataset.num_tokens, bias=False), \n","         )\n","        nn_model.type(torch.FloatTensor)\n","        nn_model.to(device)\n","\n","        optimizer = optim.SGD(nn_model.parameters(), lr=learning_rate, weight_decay=reg_strength)\n","        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n","        loss_, train_ = trainer_func(nn_model, dataset, train_loader, optimizer, scheduler, num_epochs)\n","        loss_history.append(loss_[-1])\n","        train_history.append(train_[-1])\n","        reg_strength_history.append(reg_strength)\n","        learning_rate_history.append(learning_rate)\n","        \n","    approx_reg_strength = reg_strength_history[np.argmin(loss_history)]\n","    approx_learning_rate = learning_rate_history[np.argmin(loss_history)]\n","    approx_loss = np.min(loss_history)\n","    print('best loss achieved: %f at reg_strength %f and learning_rate  %f' % (approx_loss, approx_reg_strength, approx_learning_rate))\n","    return approx_loss, approx_reg_strength, approx_learning_rate \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RrB6lC_NAfYM"},"source":["#@title\n","approx_loss, approx_reg_strength, approx_learning_rate = approx_loss_reg_strength_learning_rate(train_model, train_loader,  \\\n","                                            loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor), num_epochs = 5, \\\n","                                            num_experiments = 30, reg_strength_10_exp = (-6, -1), learning_rate_10_exp = (-1, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDp2YdKA-fHb"},"source":["#@title\n","# Visualize training graphs\n","plt.subplot(211)\n","plt.plot(train_history)\n","plt.subplot(212)\n","plt.plot(loss_history) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CmMl2EG8-fHe"},"source":["# Визуализируем вектора для разного вида слов до и после тренировки\n","\n","В случае успешной тренировки вы должны увидеть как вектора слов разных типов (например, знаков препинания, предлогов и остальных) разделяются семантически.\n","\n","Студенты - в качестве выполненного задания присылайте notebook с диаграммами!"]},{"cell_type":"code","metadata":{"id":"GxIp5ryxHo7Y","executionInfo":{"status":"ok","timestamp":1637340868042,"user_tz":-120,"elapsed":278,"user":{"displayName":"Олександр Анатоліїович Савченко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFxEhCEDXBdXMW8HVBZvI3bm-ax-lV7gibeoHr=s64","userId":"06649945919620007057"}}},"source":["trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n","assert trained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n","assert trained_output_vectors.shape == (data.num_tokens(), wordvec_dim)\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0Z5t7iQ-fHf"},"source":["def visualize_vectors(input_vectors, output_vectors, title=''):\n","    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n","    full_vectors = full_vectors.cpu()\n","    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors)\n","\n","    # Helpful words form CS244D example\n","    # http://cs224d.stanford.edu/assignment1/index.html\n","    visualize_words = {'green': [\"the\", \"a\", \"an\"], \n","                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"], \n","                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \n","                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n","                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n","                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n","                     }\n","\n","    plt.figure(figsize=(7,7))\n","    plt.suptitle(title)\n","    for color, words in visualize_words.items():\n","        points = np.array([wordvec_embedding[data.index_by_token[w]] for w in words])\n","        for i, word in enumerate(words):\n","            plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n","        plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n","\n","visualize_vectors(untrained_input_vectors, untrained_output_vectors, \"Untrained word vectors\")\n","visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"],"execution_count":null,"outputs":[]}]}