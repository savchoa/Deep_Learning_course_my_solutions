{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 5.2 - Word2Vec with Negative Sampling\n",
    "\n",
    "В этом задании мы натренируем свои версию word vectors с negative sampling на том же небольшом датасете.\n",
    "\n",
    "\n",
    "Несмотря на то, что основная причина использования Negative Sampling - улучшение скорости тренировки word2vec, в нашем игрушечном примере мы **не требуем** улучшения производительности. Мы используем negative sampling просто как дополнительное упражнение для знакомства с PyTorch.\n",
    "\n",
    "Перед запуском нужно запустить скрипт `download_data.sh`, чтобы скачать данные.\n",
    "\n",
    "Датасет и модель очень небольшие, поэтому это задание можно выполнить и без GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll use Principal Component Analysis (PCA) to visualize word vectors,\n",
    "# so make sure you install dependencies from requirements.txt!\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 19538\n",
      "seconds ['can', 'tiresome', 'jesse', 'helms']\n",
      "innuendoes ['sexual', 'abound']\n",
      "career ['deniro', 'once', 'grand', 'beach']\n",
      "yorkers ['tangled', 'particular', 'touched', 'unprecedented']\n",
      "surprises ['offers', 'few']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class StanfordTreeBank:\n",
    "    '''\n",
    "    Wrapper for accessing Stanford Tree Bank Dataset\n",
    "    https://nlp.stanford.edu/sentiment/treebank.html\n",
    "    \n",
    "    Parses dataset, gives each token and index and provides lookups\n",
    "    from string token to index and back\n",
    "    \n",
    "    Allows to generate random context with sampling strategy described in\n",
    "    word2vec paper:\n",
    "    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.index_by_token = {} # map of string -> token index\n",
    "        self.token_by_index = []\n",
    "\n",
    "        self.sentences = []\n",
    "\n",
    "        self.token_freq = {}\n",
    "        \n",
    "        self.token_reject_by_index = None\n",
    "\n",
    "    def load_dataset(self, folder):\n",
    "        filename = os.path.join(folder, \"datasetSentences.txt\")\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "            l = f.readline() # skip the first line\n",
    "            \n",
    "            for l in f:\n",
    "                splitted_line = l.strip().split()\n",
    "                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n",
    "                    \n",
    "                self.sentences.append(words)\n",
    "                for word in words:\n",
    "                    if word in self.token_freq:\n",
    "                        self.token_freq[word] +=1 \n",
    "                    else:\n",
    "                        index = len(self.token_by_index)\n",
    "                        self.token_freq[word] = 1\n",
    "                        self.index_by_token[word] = index\n",
    "                        self.token_by_index.append(word)\n",
    "        self.compute_token_prob()\n",
    "                        \n",
    "    def compute_token_prob(self):\n",
    "        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n",
    "        words_freq = words_count / np.sum(words_count)\n",
    "        \n",
    "        # Following sampling strategy from word2vec paper\n",
    "        self.token_reject_by_index = 1- np.sqrt(1e-5/words_freq)\n",
    "    \n",
    "    def check_reject(self, word):\n",
    "        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n",
    "        \n",
    "    def get_random_context(self, context_length=5):\n",
    "        \"\"\"\n",
    "        Returns tuple of center word and list of context words\n",
    "        \"\"\"\n",
    "        sentence_sampled = []\n",
    "        while len(sentence_sampled) <= 2:\n",
    "            sentence_index = np.random.randint(len(self.sentences)) \n",
    "            sentence = self.sentences[sentence_index]\n",
    "            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n",
    "    \n",
    "        center_word_index = np.random.randint(len(sentence_sampled))\n",
    "        \n",
    "        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n",
    "        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n",
    "        \n",
    "        return sentence_sampled[center_word_index], words_before+words_after\n",
    "    \n",
    "    def num_tokens(self):\n",
    "        return len(self.token_by_index)\n",
    "        \n",
    "data = StanfordTreeBank()\n",
    "data.load_dataset(\"stanfordSentimentTreebank/stanfordSentimentTreebank/\")\n",
    "\n",
    "print(\"Num tokens:\", data.num_tokens())\n",
    "for i in range(5):\n",
    "    center_word, other_words = data.get_random_context(5)\n",
    "    print(center_word, other_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset для Negative Sampling должен быть немного другим\n",
    "\n",
    "Как и прежде, Dataset должен сгенерировать много случайных контекстов и превратить их в сэмплы для тренировки.\n",
    "\n",
    "Здесь мы реализуем прямой проход модели сами, поэтому выдавать данные можно в удобном нам виде.\n",
    "Напоминаем, что в случае negative sampling каждым сэмплом является:\n",
    "- вход: слово в one-hot представлении\n",
    "- выход: набор из одного целевого слова и K других случайных слов из словаря.\n",
    "Вместо softmax + cross-entropy loss, сеть обучается через binary cross-entropy loss - то есть, предсказывает набор бинарных переменных, для каждой из которых функция ошибки считается независимо.\n",
    "\n",
    "Для целевого слова бинарное предсказание должно быть позитивным, а для K случайных слов - негативным.\n",
    "\n",
    "Из набора слово-контекст создается N сэмплов (где N - количество слов в контексте), в каждом из них K+1 целевых слов, для только одного из которых предсказание должно быть позитивным.\n",
    "Например, для K=2:\n",
    "\n",
    "Слово: `orders` и контекст: `['love', 'nicest', 'to', '50-year']` создадут 4 сэмпла:\n",
    "- input: `orders`, target: `[love: 1, any: 0, rose: 0]`\n",
    "- input: `orders`, target: `[nicest: 1, fool: 0, grass: 0]`\n",
    "- input: `orders`, target: `[to: 1, -: 0, the: 0]`\n",
    "- input: `orders`, target: `[50-year: 1, ?: 0, door: 0]`\n",
    "\n",
    "Все слова на входе и на выходе закодированы через one-hot encoding, с размером вектора равным количеству токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample - input: 10927, output indices: tensor([  803,  6728,  3508,  9825, 10020,   578,  1099, 13574, 16968, 19280,\n",
      "        18837], dtype=torch.int32), output target: tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "num_negative_samples = 10\n",
    "\n",
    "class Word2VecNegativeSampling(Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset for Word2Vec with Negative Sampling.\n",
    "    Accepts StanfordTreebank as data and is able to generate dataset based on\n",
    "    a number of random contexts\n",
    "    '''\n",
    "    def __init__(self, data, num_negative_samples, num_contexts=30000):\n",
    "        '''\n",
    "        Initializes Word2VecNegativeSampling, but doesn't generate the samples yet\n",
    "        (for that, use generate_dataset)\n",
    "        Arguments:\n",
    "        data - StanfordTreebank instace\n",
    "        num_negative_samples - number of negative samples to generate in addition to a positive one\n",
    "        num_contexts - number of random contexts to use when generating a dataset\n",
    "        '''\n",
    "        # TODO: Implement what you need for other methods!\n",
    "        self.data = data\n",
    "        self.num_contexts = num_contexts\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        self.num_tokens = self.data.num_tokens()\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        \n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        '''\n",
    "        Generates dataset samples from random contexts\n",
    "        Note: there will be more samples than contexts because every context\n",
    "        can generate more than one sample\n",
    "        '''\n",
    "        # TODO: Implement generating the dataset\n",
    "        # You should sample num_contexts contexts from the data and turn them into samples\n",
    "        # Note you will have several samples from one context\n",
    "        q=0\n",
    "        for i in range(self.num_contexts):\n",
    "            center_word, other_words = self.data.get_random_context()\n",
    "            \n",
    "            for word in other_words:\n",
    "                q+=1\n",
    "#                 print(\"%f SAMPLE\" % (q))\n",
    "                word_indeces_current_output_dict = []\n",
    "                rand_words = [] \n",
    "                self.input.append(center_word)\n",
    "                current_output_dict ={}\n",
    "                current_output_dict[word] = 1\n",
    "                word_indeces_current_output_dict.append(self.data.index_by_token[word])\n",
    "#                 print(word_indeces_current_output_dict)\n",
    "                for j in range(self.num_negative_samples):\n",
    "                    rand_word_index = np.random.randint(self.num_tokens)\n",
    "#                     print(rand_word_index)\n",
    "                    while rand_word_index in word_indeces_current_output_dict:\n",
    "                        rand_word_index = np.random.randint(self.num_tokens)\n",
    "#                     print(rand_word_index)    \n",
    "                    word_indeces_current_output_dict.append(rand_word_index)\n",
    "#                     print(word_indeces_current_output_dict)\n",
    "                    rand_word = self.data.token_by_index[rand_word_index]\n",
    "                     \n",
    "        \n",
    "        \n",
    "        \n",
    "                    rand_words.append(rand_word)\n",
    "                    current_output_dict[rand_word] = 0 \n",
    "                self.output.append(current_output_dict)\n",
    "#                 print(rand_words, word)\n",
    "#                 print(len(current_output_dict))\n",
    "             \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns total number of samples\n",
    "        '''\n",
    "        # TODO: Return the number of samples\n",
    "        return len(self.input)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns i-th sample\n",
    "        \n",
    "        Return values:\n",
    "        input_vector - index of the input word (not torch.Tensor!)\n",
    "        output_indices - torch.Tensor of indices of the target words. Should be 1+num_negative_samples.\n",
    "        output_target - torch.Tensor with float targets for the training. Should be the same size as output_indices\n",
    "                        and have 1 for the context word and 0 everywhere else\n",
    "        '''\n",
    "        # TODO: Generate tuple of 3 return arguments for i-th sample\n",
    "        input_word = self.input[index]\n",
    "        input_vector = self.data.index_by_token[input_word]\n",
    "        output_dict = self.output[index]\n",
    "        output_indices = torch.Tensor([self.data.index_by_token[output_word] for output_word in output_dict.keys()]).int()\n",
    "        output_target = torch.Tensor([output_target for output_target in output_dict.values()]).int()\n",
    "                      \n",
    "        return input_vector, output_indices, output_target\n",
    "        \n",
    "\n",
    "dataset = Word2VecNegativeSampling(data, num_negative_samples, 10000)\n",
    "dataset.generate_dataset()\n",
    "input_vector, output_indices, output_target = dataset[0]\n",
    "\n",
    "print(\"Sample - input: %s, output indices: %s, output target: %s\" % (input_vector, output_indices, output_target)) # target should be able to convert to int\n",
    "assert isinstance(output_indices, torch.Tensor)\n",
    "assert output_indices.shape[0] == num_negative_samples+1\n",
    "\n",
    "assert isinstance(output_target, torch.Tensor)\n",
    "assert output_target.shape[0] == num_negative_samples+1\n",
    "assert torch.sum(output_target) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем модель\n",
    "\n",
    "Для нашей задачи нам придется реализовать свою собственную PyTorch модель.\n",
    "Эта модель реализует свой собственный прямой проход (forward pass), который получает на вход индекс входного слова и набор индексов для выходных слов. \n",
    "\n",
    "Как всегда, на вход приходит не один сэмпл, а целый batch.  \n",
    "Напомним, что цели улучшить скорость тренировки у нас нет, достаточно чтобы она сходилась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2VecNegativeSamples(\n",
       "  (input): Linear(in_features=19538, out_features=10, bias=False)\n",
       "  (output): Linear(in_features=10, out_features=19538, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the usual PyTorch structures\n",
    "dataset = Word2VecNegativeSampling(data, num_negative_samples, 30000)\n",
    "dataset.generate_dataset()\n",
    "\n",
    "# As before, we'll be training very small word vectors!\n",
    "wordvec_dim = 10\n",
    "\n",
    "class Word2VecNegativeSamples(nn.Module):\n",
    "    def __init__(self, num_tokens):\n",
    "        super(Word2VecNegativeSamples, self).__init__()\n",
    "        self.input = nn.Linear(num_tokens, 10, bias=False)\n",
    "        self.output = nn.Linear(10, num_tokens, bias=False)\n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "    def forward(self, input_index_batch, output_indices_batch):\n",
    "        '''\n",
    "        Implements forward pass with negative sampling\n",
    "        \n",
    "        Arguments:\n",
    "        input_index_batch - Tensor of ints, shape: (batch_size, ), indices of input words in the batch\n",
    "        output_indices_batch - Tensor if ints, shape: (batch_size, num_negative_samples+1),\n",
    "                                indices of the target words for every sample\n",
    "                                \n",
    "        Returns:\n",
    "        predictions - Tensor of floats, shape: (batch_size, num_negative_samples+1)\n",
    "        '''\n",
    "        predictions = torch.Tensor([]).float().requires_grad_()\n",
    "        \n",
    "#         predictions = torch.zeros([len(input_index_batch), output_indices_batch.shape[1]]).float()\n",
    "        \n",
    "        # TODO Implement forward pass\n",
    "        # Hint: You can use for loop to go over all samples on the batch,\n",
    "        # run every sample indivisually and then use\n",
    "        # torch.stack or torch.cat to produce the final result\n",
    "#         for n_sample in range(len(input_index_batch)):\n",
    "#             input_vector = torch.zeros([1, self.num_tokens]).float()\n",
    "#             input_word_index = input_index_batch[n_sample]\n",
    "#             input_vector[0, input_word_index] = 1\n",
    "\n",
    "#             pred_1 = self.input(input_vector)\n",
    "\n",
    "#             pred_2 = self.output(pred_1)\n",
    "\n",
    "#             output_indeces = output_indices_batch[n_sample,:]\n",
    "\n",
    "#             output_vector = torch.index_select(pred_2, 1, output_indeces) \n",
    "            \n",
    "#             predictions = torch.cat((predictions, output_vector), 0) \n",
    "        \n",
    "    \n",
    "    \n",
    "              \n",
    "#         input_ = torch.zeros([len(input_index_batch), self.num_tokens]).float()\n",
    "#         input_[range(len(input_index_batch)), input_index_batch ] = 1\n",
    "#         pred_1 = self.input(input_)\n",
    "#         pred_2 = self.output(pred_1)\n",
    "#         for row in range(len(input_index_batch)):\n",
    "#             predictions[row] = pred_2[row][output_indices_batch[row].tolist()]\n",
    "    \n",
    "        output_indices_batch = output_indices_batch.type('torch.LongTensor') \n",
    "        w1 = torch.t([(param*1) for param in self.input.parameters()][0])\n",
    "#         print(w1.requires_grad, w1.is_leaf)\n",
    "        w2 = [(param*1) for param in self.output.parameters()][0]\n",
    "#         print(w2.requires_grad, w2.is_leaf)\n",
    "        for n_sample in range(output_indices_batch.shape[0]):\n",
    "            m1 = w1[input_index_batch[n_sample]].reshape(1, w1.shape[1])\n",
    "#             print(m1.requires_grad, m1.is_leaf)\n",
    "            output_ind = output_indices_batch[n_sample] \n",
    "            m2 = torch.t(w2[output_ind])\n",
    "#             print(m2.requires_grad, m2.is_leaf)\n",
    "            pred = torch.matmul(m1,m2)\n",
    "#             print(pred.requires_grad, pred.is_leaf)\n",
    "            predictions = torch.cat((predictions, pred), 0)\n",
    "#             print(predictions.requires_grad, predictions.is_leaf)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return predictions\n",
    "                 \n",
    "    \n",
    "nn_model = Word2VecNegativeSamples(data.num_tokens())\n",
    "nn_model.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n"
     ]
    }
   ],
   "source": [
    "print([param.grad for param in nn_model.input.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 3.3271e-03,  2.5577e-04,  2.9504e-03,  ...,  1.4665e-03,\n",
       "           3.4930e-03, -3.2840e-03],\n",
       "         [ 2.3258e-03,  6.4329e-03, -2.5227e-03,  ...,  5.5142e-03,\n",
       "          -1.6655e-03, -5.1021e-03],\n",
       "         [ 3.9641e-03, -7.1280e-03,  4.8454e-03,  ..., -4.4003e-03,\n",
       "           4.7619e-03, -1.1641e-03],\n",
       "         ...,\n",
       "         [-4.7400e-03,  5.0435e-03, -1.9362e-03,  ...,  5.9505e-03,\n",
       "          -9.9611e-04, -3.6416e-03],\n",
       "         [ 4.1198e-03,  6.2494e-03,  6.7986e-03,  ...,  1.6582e-03,\n",
       "           4.8085e-03, -2.3875e-03],\n",
       "         [-5.4196e-03, -2.9594e-03,  4.8497e-03,  ..., -5.3968e-03,\n",
       "           2.2654e-03, -1.4168e-05]], requires_grad=True)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param for param in nn_model.input.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.3271e-03,  2.5577e-04,  2.9504e-03,  ...,  1.4665e-03,\n",
       "          3.4930e-03, -3.2840e-03],\n",
       "        [ 2.3258e-03,  6.4329e-03, -2.5227e-03,  ...,  5.5142e-03,\n",
       "         -1.6655e-03, -5.1021e-03],\n",
       "        [ 3.9641e-03, -7.1280e-03,  4.8454e-03,  ..., -4.4003e-03,\n",
       "          4.7619e-03, -1.1641e-03],\n",
       "        ...,\n",
       "        [-4.7400e-03,  5.0435e-03, -1.9362e-03,  ...,  5.9505e-03,\n",
       "         -9.9611e-04, -3.6416e-03],\n",
       "        [ 4.1198e-03,  6.2494e-03,  6.7986e-03,  ...,  1.6582e-03,\n",
       "          4.8085e-03, -2.3875e-03],\n",
       "        [-5.4196e-03, -2.9594e-03,  4.8497e-03,  ..., -5.3968e-03,\n",
       "          2.2654e-03, -1.4168e-05]], requires_grad=True)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param for param in nn_model.parameters()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0041, -0.0050, -0.0018,  ..., -0.0038,  0.0002, -0.0071],\n",
       "        [ 0.0012,  0.0040, -0.0033,  ..., -0.0005, -0.0047,  0.0017],\n",
       "        [-0.0004, -0.0020, -0.0065,  ..., -0.0034, -0.0045, -0.0010],\n",
       "        ...,\n",
       "        [ 0.0014, -0.0013,  0.0063,  ...,  0.0027,  0.0009, -0.0031],\n",
       "        [-0.0050,  0.0058, -0.0068,  ...,  0.0018, -0.0025, -0.0047],\n",
       "        [-0.0033,  0.0009, -0.0057,  ..., -0.0042, -0.0026, -0.0016]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 =[(param*1) for param in nn_model.input.parameters()][0]\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0010,  0.0012,  0.0031,  ..., -0.0030,  0.0028, -0.0036],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3624e-03,  5.8675e-03, -3.8278e-03,  ...,  2.5021e-03,\n",
       "          5.1014e-03, -5.8364e-04],\n",
       "        [ 1.3429e-03, -6.4820e-03,  6.3316e-03,  ...,  2.2729e-03,\n",
       "          4.5435e-04, -1.6621e-03],\n",
       "        [-4.2655e-03, -5.3550e-03,  7.1297e-04,  ..., -6.5988e-03,\n",
       "          6.1779e-06,  5.6762e-03],\n",
       "        ...,\n",
       "        [-3.0516e-03, -6.2825e-03, -1.4608e-03,  ...,  1.2444e-03,\n",
       "          5.8165e-03,  2.0040e-04],\n",
       "        [ 4.0824e-03, -5.0410e-04,  2.2421e-03,  ..., -3.8134e-03,\n",
       "         -3.4053e-03, -5.9445e-03],\n",
       "        [-4.4921e-03, -4.7227e-03, -3.1155e-03,  ..., -4.8462e-03,\n",
       "         -3.5674e-04, -2.1327e-03]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = torch.t([param.data for param in nn_model.parameters()][0])\n",
    "w1.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'is_leaf_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f701e3aed427>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_leaf_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"False\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'is_leaf_'"
     ]
    }
   ],
   "source": [
    "w1.is_leaf_(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.retain_grad of tensor([[ 1.3624e-03,  5.8675e-03, -3.8278e-03,  ...,  2.5021e-03,\n",
       "          5.1014e-03, -5.8364e-04],\n",
       "        [ 1.3429e-03, -6.4820e-03,  6.3316e-03,  ...,  2.2729e-03,\n",
       "          4.5435e-04, -1.6621e-03],\n",
       "        [-4.2655e-03, -5.3550e-03,  7.1297e-04,  ..., -6.5988e-03,\n",
       "          6.1779e-06,  5.6762e-03],\n",
       "        ...,\n",
       "        [-3.0516e-03, -6.2825e-03, -1.4608e-03,  ...,  1.2444e-03,\n",
       "          5.8165e-03,  2.0040e-04],\n",
       "        [ 4.0824e-03, -5.0410e-04,  2.2421e-03,  ..., -3.8134e-03,\n",
       "         -3.4053e-03, -5.9445e-03],\n",
       "        [-4.4921e-03, -4.7227e-03, -3.1155e-03,  ..., -4.8462e-03,\n",
       "         -3.5674e-04, -2.1327e-03]], requires_grad=True)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.retain_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr = [param.grad for param in nn_model.input.parameters()]\n",
    "gr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0003,  0.0024, -0.0007, -0.0016, -0.0054, -0.0020,  0.0032, -0.0066,\n",
       "          0.0017, -0.0051]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = w1[0].reshape(1, 10)\n",
    "m1.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ind = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(param.data*1).is_leaf for param in nn_model.parameters()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = [param.data for param in nn_model.parameters()][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3624e-03,  5.8675e-03, -3.8278e-03,  ...,  2.5021e-03,\n",
       "          5.1014e-03, -5.8364e-04],\n",
       "        [ 1.3429e-03, -6.4820e-03,  6.3316e-03,  ...,  2.2729e-03,\n",
       "          4.5435e-04, -1.6621e-03],\n",
       "        [-4.2655e-03, -5.3550e-03,  7.1297e-04,  ..., -6.5988e-03,\n",
       "          6.1779e-06,  5.6762e-03],\n",
       "        ...,\n",
       "        [-3.0516e-03, -6.2825e-03, -1.4608e-03,  ...,  1.2444e-03,\n",
       "          5.8165e-03,  2.0040e-04],\n",
       "        [ 4.0824e-03, -5.0410e-04,  2.2421e-03,  ..., -3.8134e-03,\n",
       "         -3.4053e-03, -5.9445e-03],\n",
       "        [-4.4921e-03, -4.7227e-03, -3.1155e-03,  ..., -4.8462e-03,\n",
       "         -3.5674e-04, -2.1327e-03]], requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1110, -0.2169, -0.0652,  ...,  0.0942,  0.0219,  0.1264],\n",
       "        [ 0.2457, -0.0487, -0.0121,  ...,  0.1763, -0.0029, -0.1987],\n",
       "        [ 0.0368, -0.0141, -0.0812,  ...,  0.0100, -0.0989, -0.1545],\n",
       "        ...,\n",
       "        [ 0.1195,  0.0206, -0.0482,  ..., -0.2491,  0.2809, -0.1351],\n",
       "        [ 0.0267, -0.1937,  0.2292,  ...,  0.0502,  0.1929, -0.2277],\n",
       "        [ 0.2141,  0.1465, -0.2330,  ...,  0.0186, -0.2122,  0.2607]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0010,  0.0012,  0.0031,  ..., -0.0030,  0.0028, -0.0036],\n",
       "         [ 0.0071, -0.0019, -0.0023,  ..., -0.0007,  0.0054,  0.0046],\n",
       "         [-0.0023,  0.0058,  0.0054,  ...,  0.0067,  0.0019,  0.0042],\n",
       "         ...,\n",
       "         [-0.0049,  0.0022,  0.0042,  ...,  0.0026, -0.0062, -0.0048],\n",
       "         [-0.0035, -0.0061,  0.0059,  ...,  0.0010, -0.0044, -0.0035],\n",
       "         [-0.0066, -0.0056,  0.0057,  ...,  0.0058,  0.0051,  0.0066]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr = [param for param in nn_model.input.parameters()]\n",
    "gr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x00000000056A25F0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.input.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1468,  0.1707,  0.1678,  0.0233, -0.1235, -0.1545, -0.0900, -0.2851,\n",
       "          0.2585,  0.2899,  0.0768],\n",
       "        [-0.2583,  0.2804, -0.1437,  0.0336, -0.2281,  0.2534,  0.0823,  0.0826,\n",
       "          0.0594,  0.0964, -0.2174],\n",
       "        [-0.1407, -0.0622,  0.0738,  0.2529, -0.2787,  0.0425,  0.2343,  0.1659,\n",
       "          0.3078,  0.0120,  0.0084],\n",
       "        [ 0.0897,  0.2059,  0.0433,  0.3120,  0.1637, -0.2694, -0.2415, -0.2841,\n",
       "          0.2193,  0.1178,  0.1219],\n",
       "        [ 0.0712, -0.2343, -0.2082,  0.1549,  0.1034,  0.2707,  0.2247,  0.2907,\n",
       "         -0.3047, -0.1955,  0.2333],\n",
       "        [-0.0445, -0.0904, -0.3132,  0.1190,  0.1084, -0.1875, -0.2409, -0.0674,\n",
       "          0.2143, -0.1601, -0.1335],\n",
       "        [ 0.1465, -0.2202,  0.3016,  0.1298,  0.2201,  0.0734,  0.0788,  0.3048,\n",
       "         -0.0270,  0.1450, -0.0641],\n",
       "        [ 0.1020, -0.2906,  0.2984, -0.2794,  0.1390, -0.1881,  0.1639, -0.1721,\n",
       "         -0.2379, -0.0068,  0.2243],\n",
       "        [ 0.2297,  0.2350, -0.0716, -0.1292,  0.1619,  0.3137,  0.1244, -0.1953,\n",
       "          0.0133,  0.2519,  0.0315],\n",
       "        [ 0.0270, -0.1388,  0.2105,  0.0767, -0.0151, -0.3069, -0.1191, -0.1601,\n",
       "         -0.1840,  0.2470,  0.2963]], grad_fn=<TBackward>)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = torch.t(w2[output_ind])\n",
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.7658e-04,  4.0723e-03, -9.3365e-04, -3.4686e-05, -1.2278e-03,\n",
       "          3.5358e-03, -2.7866e-04,  1.8118e-03,  3.1768e-03,  9.8562e-04,\n",
       "         -4.8958e-03]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.matmul(m1, m2)\n",
    "pred.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], requires_grad=True)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.Tensor([]).float().requires_grad_()\n",
    "preds.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.7658e-04,  4.0723e-03, -9.3365e-04, -3.4686e-05, -1.2278e-03,\n",
       "          3.5358e-03, -2.7866e-04,  1.8118e-03,  3.1768e-03,  9.8562e-04,\n",
       "         -4.8958e-03],\n",
       "        [-8.7658e-04,  4.0723e-03, -9.3365e-04, -3.4686e-05, -1.2278e-03,\n",
       "          3.5358e-03, -2.7866e-04,  1.8118e-03,  3.1768e-03,  9.8562e-04,\n",
       "         -4.8958e-03]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.cat((preds, pred), 0)\n",
    "preds.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.7658e-04,  4.0723e-03, -9.3365e-04, -3.4686e-05, -1.2278e-03,\n",
       "          3.5358e-03, -2.7866e-04,  1.8118e-03,  3.1768e-03,  9.8562e-04,\n",
       "         -4.8958e-03],\n",
       "        [-8.7658e-04,  4.0723e-03, -9.3365e-04, -3.4686e-05, -1.2278e-03,\n",
       "          3.5358e-03, -2.7866e-04,  1.8118e-03,  3.1768e-03,  9.8562e-04,\n",
       "         -4.8958e-03]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indices_batch_shape_1 = 11\n",
    "output_indices_batch_shape_0 = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.t([param.data for param in self.parameters()][0])\n",
    "w2 = [param.data for param in self.parameters()][1]\n",
    "for n_sample in range(output_indices_batch.shape[0]):\n",
    "    \n",
    "    m1 = w1[input_index_batch[n_sample]]\n",
    "    output_ind = output_indices_batch[n_sample] \n",
    "    m2 = torch.t(w2[output_ind])\n",
    "    pred = torch.matmul(m1,m2) \n",
    "    print(pred.shape)\n",
    "    predictions = torch.cat((predictions, pred), 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_sample in range(output_indices_batch_shape_0):\n",
    "    m1 = torch.t([param.data for param in nn_model.parameters()][0])[n_sample]\n",
    "    output_ind = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # output_indeces = output_indices_batch[n_sample]\n",
    "    m2 = torch.t([param.data for param in nn_model.parameters()][1][output_ind])\n",
    "    pred = torch.matmul(m1,m2)\n",
    "    preds = torch.cat((preds, pred), 0)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0051, -0.0040, -0.0008, -0.0022, -0.0008,  0.0020,  0.0006,  0.0029,\n",
       "        -0.0022,  0.0033, -0.0016, -0.0006,  0.0020,  0.0005, -0.0031, -0.0007,\n",
       "         0.0005, -0.0006,  0.0044, -0.0015, -0.0002,  0.0011, -0.0072, -0.0026,\n",
       "        -0.0010,  0.0012, -0.0032,  0.0008,  0.0031, -0.0007, -0.0023,  0.0043,\n",
       "        -0.0015])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0072, -0.0026, -0.0010,  0.0012, -0.0032,  0.0008,  0.0031, -0.0007,\n",
       "        -0.0023,  0.0043, -0.0015])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.Tensor([]).float(), pred), 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 19538])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t([param.data for param in nn_model.parameters()][1])[:, torch.tensor([0, 1, 3])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12],\n",
       "        [13, 14, 15, 16]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = torch.tensor([[0, 1], [1, 2], [2, 3], [0, 1]])\n",
    "f.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = torch.zeros([4, 2])\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.],\n",
       "        [ 6.,  7.],\n",
       "        [11., 12.],\n",
       "        [13., 14.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in range(4):\n",
    "    pr[row] = q[row][f[row]]\n",
    "pr    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9088e-03, -6.4707e-04, -5.1057e-03,  ..., -2.3880e-03,\n",
       "          4.3921e-03, -1.2205e-03],\n",
       "        [-7.5033e-04, -1.4440e-03,  6.4289e-03,  ...,  2.0097e-04,\n",
       "         -6.1794e-03, -1.9751e-03],\n",
       "        [-4.3262e-03, -6.8158e-03, -3.4356e-03,  ..., -2.7210e-04,\n",
       "         -2.1620e-03,  1.9391e-03],\n",
       "        ...,\n",
       "        [ 2.7808e-03,  3.4335e-03, -5.6131e-03,  ...,  4.6939e-06,\n",
       "         -7.2159e-05,  5.1812e-03],\n",
       "        [ 1.2654e-03, -2.8020e-03,  7.6238e-04,  ...,  3.4002e-03,\n",
       "         -6.3169e-03, -3.6801e-03],\n",
       "        [ 7.0225e-03, -5.1173e-03, -1.3122e-03,  ...,  2.3128e-03,\n",
       "         -4.0132e-04,  7.1506e-03]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param for param in nn_model.input.parameters()][0].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9088e-03, -6.4707e-04, -5.1057e-03,  ..., -2.3880e-03,\n",
       "          4.3921e-03, -1.2205e-03],\n",
       "        [-7.5033e-04, -1.4440e-03,  6.4289e-03,  ...,  2.0097e-04,\n",
       "         -6.1794e-03, -1.9751e-03],\n",
       "        [-4.3262e-03, -6.8158e-03, -3.4356e-03,  ..., -2.7210e-04,\n",
       "         -2.1620e-03,  1.9391e-03],\n",
       "        ...,\n",
       "        [ 2.7808e-03,  3.4335e-03, -5.6131e-03,  ...,  4.6939e-06,\n",
       "         -7.2159e-05,  5.1812e-03],\n",
       "        [ 1.2654e-03, -2.8020e-03,  7.6238e-04,  ...,  3.4002e-03,\n",
       "         -6.3169e-03, -3.6801e-03],\n",
       "        [ 7.0225e-03, -5.1173e-03, -1.3122e-03,  ...,  2.3128e-03,\n",
       "         -4.0132e-04,  7.1506e-03]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param for param in nn_model.input.parameters()][0].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_vectors(nn_model):\n",
    "    '''\n",
    "    Extracts word vectors from the model\n",
    "    \n",
    "    Returns:\n",
    "    input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    '''\n",
    "    # TODO: Implement extracting word vectors from param weights\n",
    "    # return tuple of input vectors and output vectos\n",
    "    input_vectors = torch.t([param.data for param in nn_model.parameters()][0].clone().detach())\n",
    "    output_vectors = [param.data for param in nn_model.parameters()][1].clone().detach()\n",
    "    return input_vectors, output_vectors\n",
    "\n",
    "untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n",
    "assert untrained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert untrained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neg_sample(model, dataset, train_loader, optimizer, scheduler, num_epochs):    \n",
    "    '''\n",
    "    Trains word2vec with negative samples on and regenerating dataset every epoch\n",
    "    \n",
    "    Returns:\n",
    "    loss_history, train_history\n",
    "    '''\n",
    "    loss = nn.BCEWithLogitsLoss().type(torch.FloatTensor)\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        dataset.generate_dataset()\n",
    "        \n",
    "        # TODO: Implement training using negative samples\n",
    "        # You can estimate accuracy by comparing prediction values with 0\n",
    "        # And don't forget to step the scheduler!\n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (input_index_batch, output_indices_batch, output_target_batch) in enumerate(train_loader):\n",
    "            prediction = model(input_index_batch, output_indices_batch)    \n",
    "            loss_value = loss(prediction, output_target_batch.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            \n",
    "#             if i_step == 0:\n",
    "            w_1=[param.data for param in nn_model.input.parameters()][0].clone().detach()\n",
    "            print(w_1)\n",
    "#                 \n",
    "            \n",
    "            optimizer.step()\n",
    "#             if i_step == 0:\n",
    "            w_2=[param.data for param in nn_model.input.parameters()][0].clone().detach()\n",
    "            print(w_2)\n",
    "            print(torch.equal(w_1, w_2))\n",
    "            \n",
    "        \n",
    "            \n",
    "            r_c = prediction[:, 0]>=0\n",
    "            w_c = prediction[:, 1:]<0\n",
    "            mask = torch.cat((r_c.reshape(input_index_batch.shape[0],1), w_c), 1)\n",
    "            correct_samples += int(torch.sum(torch.sum(mask, 1) == mask.shape[1]))\n",
    "            total_samples += mask.shape[0]\n",
    "            loss_accum += loss_value\n",
    "\n",
    "        \n",
    "            if i_step == 3:\n",
    "                break\n",
    "        if epoch == 0:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        \n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        scheduler.step()      \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"Average loss: %f, Train accuracy: %f\" % (ave_loss, train_accuracy))\n",
    "        \n",
    "    return loss_history, train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165774"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = [param.data for param in nn_model.input.parameters()][0].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0050, -0.0017,  0.0066,  ..., -0.0041, -0.0013,  0.0042],\n",
      "        [ 0.0059,  0.0061,  0.0060,  ..., -0.0035, -0.0029, -0.0054],\n",
      "        [-0.0053,  0.0063, -0.0012,  ...,  0.0059, -0.0015,  0.0068],\n",
      "        ...,\n",
      "        [-0.0026,  0.0002,  0.0058,  ...,  0.0068, -0.0034,  0.0037],\n",
      "        [ 0.0019,  0.0013, -0.0024,  ..., -0.0028, -0.0056, -0.0065],\n",
      "        [ 0.0030,  0.0069, -0.0020,  ...,  0.0006, -0.0040,  0.0034]])\n"
     ]
    }
   ],
   "source": [
    "print(w_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0050, -0.0017,  0.0066,  ..., -0.0041, -0.0013,  0.0042],\n",
       "        [ 0.0059,  0.0061,  0.0060,  ..., -0.0035, -0.0029, -0.0054],\n",
       "        [-0.0053,  0.0063, -0.0012,  ...,  0.0059, -0.0015,  0.0068],\n",
       "        ...,\n",
       "        [-0.0026,  0.0002,  0.0058,  ...,  0.0068, -0.0034,  0.0037],\n",
       "        [ 0.0019,  0.0013, -0.0024,  ..., -0.0028, -0.0056, -0.0065],\n",
       "        [ 0.0030,  0.0069, -0.0020,  ...,  0.0006, -0.0040,  0.0034]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.data for param in nn_model.input.parameters()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну и наконец тренировка!\n",
    "\n",
    "Добейтесь значения ошибки меньше **0.25**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's train the model!\n",
    "\n",
    "# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=2.4, weight_decay=0.000016)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0050, -0.0017,  0.0066,  ..., -0.0041, -0.0013,  0.0042],\n",
      "        [ 0.0059,  0.0061,  0.0060,  ..., -0.0035, -0.0029, -0.0054],\n",
      "        [-0.0053,  0.0063, -0.0012,  ...,  0.0059, -0.0015,  0.0068],\n",
      "        ...,\n",
      "        [-0.0026,  0.0002,  0.0058,  ...,  0.0068, -0.0034,  0.0037],\n",
      "        [ 0.0019,  0.0013, -0.0024,  ..., -0.0028, -0.0056, -0.0065],\n",
      "        [ 0.0030,  0.0069, -0.0020,  ...,  0.0006, -0.0040,  0.0034]])\n",
      "tensor([[ 0.0050, -0.0017,  0.0066,  ..., -0.0041, -0.0013,  0.0042],\n",
      "        [ 0.0059,  0.0061,  0.0060,  ..., -0.0035, -0.0029, -0.0054],\n",
      "        [-0.0053,  0.0063, -0.0012,  ...,  0.0059, -0.0015,  0.0068],\n",
      "        ...,\n",
      "        [-0.0026,  0.0002,  0.0058,  ...,  0.0068, -0.0034,  0.0037],\n",
      "        [ 0.0019,  0.0013, -0.0024,  ..., -0.0028, -0.0056, -0.0065],\n",
      "        [ 0.0030,  0.0069, -0.0020,  ...,  0.0006, -0.0040,  0.0034]])\n",
      "False\n",
      "tensor([[ 0.0050, -0.0017,  0.0066,  ..., -0.0041, -0.0013,  0.0042],\n",
      "        [ 0.0059,  0.0061,  0.0060,  ..., -0.0035, -0.0029, -0.0054],\n",
      "        [-0.0053,  0.0063, -0.0012,  ...,  0.0059, -0.0015,  0.0068],\n",
      "        ...,\n",
      "        [-0.0026,  0.0002,  0.0058,  ...,  0.0068, -0.0034,  0.0037],\n",
      "        [ 0.0019,  0.0013, -0.0024,  ..., -0.0028, -0.0056, -0.0065],\n",
      "        [ 0.0030,  0.0069, -0.0020,  ...,  0.0006, -0.0040,  0.0034]])\n",
      "tensor([[ 0.0050, -0.0017,  0.0066,  ..., -0.0041, -0.0013,  0.0042],\n",
      "        [ 0.0059,  0.0061,  0.0060,  ..., -0.0035, -0.0029, -0.0054],\n",
      "        [-0.0053,  0.0063, -0.0012,  ...,  0.0059, -0.0015,  0.0068],\n",
      "        ...,\n",
      "        [-0.0026,  0.0002,  0.0058,  ...,  0.0068, -0.0034,  0.0037],\n",
      "        [ 0.0019,  0.0013, -0.0024,  ..., -0.0028, -0.0056, -0.0065],\n",
      "        [ 0.0030,  0.0069, -0.0020,  ...,  0.0006, -0.0040,  0.0034]])\n",
      "False\n",
      "tensor([[ 0.0050, -0.0017,  0.0066,  ..., -0.0041, -0.0013,  0.0042],\n",
      "        [ 0.0059,  0.0061,  0.0060,  ..., -0.0035, -0.0029, -0.0054],\n",
      "        [-0.0053,  0.0063, -0.0012,  ...,  0.0059, -0.0015,  0.0068],\n",
      "        ...,\n",
      "        [-0.0026,  0.0002,  0.0058,  ...,  0.0068, -0.0034,  0.0037],\n",
      "        [ 0.0019,  0.0013, -0.0024,  ..., -0.0028, -0.0056, -0.0065],\n",
      "        [ 0.0030,  0.0069, -0.0020,  ...,  0.0006, -0.0040,  0.0034]])\n",
      "tensor([[ 0.0050, -0.0017,  0.0066,  ..., -0.0041, -0.0013,  0.0042],\n",
      "        [ 0.0059,  0.0061,  0.0060,  ..., -0.0035, -0.0029, -0.0054],\n",
      "        [-0.0053,  0.0063, -0.0012,  ...,  0.0059, -0.0015,  0.0068],\n",
      "        ...,\n",
      "        [-0.0026,  0.0002,  0.0058,  ...,  0.0068, -0.0034,  0.0037],\n",
      "        [ 0.0019,  0.0013, -0.0024,  ..., -0.0028, -0.0056, -0.0065],\n",
      "        [ 0.0030,  0.0069, -0.0020,  ...,  0.0006, -0.0040,  0.0034]])\n",
      "False\n",
      "tensor([[ 0.0050, -0.0017,  0.0066,  ..., -0.0041, -0.0013,  0.0042],\n",
      "        [ 0.0059,  0.0061,  0.0060,  ..., -0.0035, -0.0029, -0.0054],\n",
      "        [-0.0053,  0.0063, -0.0012,  ...,  0.0059, -0.0015,  0.0068],\n",
      "        ...,\n",
      "        [-0.0026,  0.0002,  0.0058,  ...,  0.0068, -0.0034,  0.0037],\n",
      "        [ 0.0019,  0.0013, -0.0024,  ..., -0.0028, -0.0056, -0.0065],\n",
      "        [ 0.0030,  0.0069, -0.0020,  ...,  0.0006, -0.0040,  0.0034]])\n",
      "tensor([[ 0.0050, -0.0017,  0.0066,  ..., -0.0041, -0.0013,  0.0042],\n",
      "        [ 0.0059,  0.0061,  0.0060,  ..., -0.0035, -0.0029, -0.0054],\n",
      "        [-0.0053,  0.0063, -0.0012,  ...,  0.0059, -0.0015,  0.0068],\n",
      "        ...,\n",
      "        [-0.0026,  0.0002,  0.0058,  ...,  0.0068, -0.0034,  0.0037],\n",
      "        [ 0.0019,  0.0013, -0.0024,  ..., -0.0028, -0.0056, -0.0065],\n",
      "        [ 0.0030,  0.0069, -0.0020,  ...,  0.0006, -0.0040,  0.0034]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "loss_history, train_history = train_neg_sample(nn_model, dataset, train_loader, optimizer, scheduler, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize training graphs\n",
    "plt.subplot(211)\n",
    "plt.plot(train_history)\n",
    "plt.subplot(212)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализируем вектора для разного вида слов до и после тренировки\n",
    "\n",
    "Как и ранее, в случае успешной тренировки вы должны увидеть как вектора слов разных типов (например, знаков препинания, предлогов и остальных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n",
    "assert trained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert trained_output_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "\n",
    "def visualize_vectors(input_vectors, output_vectors, title=''):\n",
    "    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n",
    "    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors)\n",
    "\n",
    "    # Helpful words form CS244D example\n",
    "    # http://cs224d.stanford.edu/assignment1/index.html\n",
    "    visualize_words = {'green': [\"the\", \"a\", \"an\"], \n",
    "                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"], \n",
    "                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \n",
    "                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n",
    "                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n",
    "                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n",
    "                     }\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.suptitle(title)\n",
    "    for color, words in visualize_words.items():\n",
    "        points = np.array([wordvec_embedding[data.index_by_token[w]] for w in words])\n",
    "        for i, word in enumerate(words):\n",
    "            plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n",
    "        plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n",
    "\n",
    "visualize_vectors(untrained_input_vectors, untrained_output_vectors, \"Untrained word vectors\")\n",
    "visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
